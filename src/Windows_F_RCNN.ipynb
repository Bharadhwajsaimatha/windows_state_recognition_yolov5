{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e499e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16926b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the XML annotations\n",
    "xml_dir = r\"C:\\Users\\gokul\\Desktop\\CSIS_SS23\\F_RCNN_dataset\\train\\labels\"\n",
    "\n",
    "# Output file path for the COCO annotations\n",
    "output_file = r'C:\\Users\\gokul\\Desktop\\CSIS_SS23\\F_RCNN_dataset\\train\\train_annotations.json'\n",
    "\n",
    "# Initialize the COCO annotations dictionary\n",
    "coco_annotations = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'categories': []\n",
    "}\n",
    "\n",
    "# Define your class labels and their corresponding IDs\n",
    "class_labels = {'window': 1}\n",
    "\n",
    "# Iterate over the XML files\n",
    "for filename in os.listdir(xml_dir):\n",
    "    if filename.endswith('.xml'):\n",
    "        xml_path = os.path.join(xml_dir, filename)\n",
    "\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract image-level information\n",
    "        image_id = len(coco_annotations['images']) + 1\n",
    "        image_width = int(root.find('size/width').text)\n",
    "        image_height = int(root.find('size/height').text)\n",
    "        image_file_name = root.find('filename').text\n",
    "\n",
    "        # Create image entry\n",
    "        image_entry = {\n",
    "            'id': image_id,\n",
    "            'width': image_width,\n",
    "            'height': image_height,\n",
    "            'file_name': image_file_name\n",
    "        }\n",
    "\n",
    "        # Append the image entry to the COCO annotations\n",
    "        coco_annotations['images'].append(image_entry)\n",
    "\n",
    "        # Extract object-level information\n",
    "        for obj in root.findall('object'):\n",
    "            class_label = obj.find('name').text\n",
    "\n",
    "            # Skip objects that are not in your class labels\n",
    "            if class_label not in class_labels:\n",
    "                continue\n",
    "\n",
    "            class_id = class_labels[class_label]\n",
    "\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = int(bbox.find('xmin').text)\n",
    "            ymin = int(bbox.find('ymin').text)\n",
    "            xmax = int(bbox.find('xmax').text)\n",
    "            ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "            # Create annotation entry\n",
    "            annotation_entry = {\n",
    "                'id': len(coco_annotations['annotations']) + 1,\n",
    "                'image_id': image_id,\n",
    "                'category_id': class_id,\n",
    "                'bbox': [xmin, ymin, xmax - xmin, ymax - ymin],\n",
    "                'area': (xmax - xmin) * (ymax - ymin),\n",
    "                'iscrowd': 0\n",
    "            }\n",
    "\n",
    "            # Append the annotation entry to the COCO annotations\n",
    "            coco_annotations['annotations'].append(annotation_entry)\n",
    "\n",
    "# Create category entries\n",
    "for class_label, class_id in class_labels.items():\n",
    "    category_entry = {\n",
    "        'id': class_id,\n",
    "        'name': class_label,\n",
    "        'supercategory': 'object'\n",
    "    }\n",
    "    coco_annotations['categories'].append(category_entry)\n",
    "\n",
    "# Save the COCO annotations to a JSON file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(coco_annotations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3753bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated for normalisation\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "# Path to the directory containing the XML annotations\n",
    "xml_dir = r\"C:\\Users\\gokul\\Desktop\\CSIS_SS23\\F_RCNN_dataset\\val\\labels\"\n",
    "\n",
    "# Output file path for the COCO annotations\n",
    "output_file = r'C:\\Users\\gokul\\Desktop\\CSIS_SS23\\F_RCNN_dataset\\val\\val_annotations.json'\n",
    "\n",
    "# Initialize the COCO annotations dictionary\n",
    "coco_annotations = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'categories': []\n",
    "}\n",
    "\n",
    "# Define your class labels and their corresponding IDs\n",
    "class_labels = {'window': 1}\n",
    "\n",
    "# Iterate over the XML files\n",
    "for filename in os.listdir(xml_dir):\n",
    "    if filename.endswith('.xml'):\n",
    "        xml_path = os.path.join(xml_dir, filename)\n",
    "\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract image-level information\n",
    "        image_id = len(coco_annotations['images']) + 1\n",
    "        image_width = int(root.find('size/width').text)\n",
    "        image_height = int(root.find('size/height').text)\n",
    "        image_file_name = root.find('filename').text\n",
    "\n",
    "        # Create image entry\n",
    "        image_entry = {\n",
    "            'id': image_id,\n",
    "            'width': image_width,\n",
    "            'height': image_height,\n",
    "            'file_name': image_file_name\n",
    "        }\n",
    "\n",
    "        # Append the image entry to the COCO annotations\n",
    "        coco_annotations['images'].append(image_entry)\n",
    "\n",
    "        # Extract object-level information\n",
    "        for obj in root.findall('object'):\n",
    "            class_label = obj.find('name').text\n",
    "\n",
    "            # Skip objects that are not in your class labels\n",
    "            if class_label not in class_labels:\n",
    "                continue\n",
    "\n",
    "            class_id = class_labels[class_label]\n",
    "\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = int(bbox.find('xmin').text) / image_width\n",
    "            ymin = int(bbox.find('ymin').text) / image_height\n",
    "            xmax = int(bbox.find('xmax').text) / image_width\n",
    "            ymax = int(bbox.find('ymax').text) / image_height\n",
    "\n",
    "            # Create annotation entry\n",
    "            annotation_entry = {\n",
    "                'id': len(coco_annotations['annotations']) + 1,\n",
    "                'image_id': image_id,\n",
    "                'category_id': class_id,\n",
    "                'bbox': [xmin, ymin, xmax - xmin, ymax - ymin],\n",
    "                'area': (xmax - xmin) * (ymax - ymin),\n",
    "                'iscrowd': 0\n",
    "            }\n",
    "\n",
    "            # Append the annotation entry to the COCO annotations\n",
    "            coco_annotations['annotations'].append(annotation_entry)\n",
    "\n",
    "# Create category entries\n",
    "for class_label, class_id in class_labels.items():\n",
    "    category_entry = {\n",
    "        'id': class_id,\n",
    "        'name': class_label,\n",
    "        'supercategory': 'object'\n",
    "    }\n",
    "    coco_annotations['categories'].append(category_entry)\n",
    "\n",
    "# Save the COCO annotations to a JSON file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(coco_annotations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70061110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Path to the training dataset annotations in COCO format\n",
    "train_annotations_path = r\"C:\\Users\\gokul\\Desktop\\CSIS_SS23\\F_RCNN_dataset\\train\\train_annotations.json\"\n",
    "\n",
    "# Path to the validation dataset annotations in COCO format\n",
    "val_annotations_path = r\"C:\\Users\\gokul\\Desktop\\CSIS_SS23\\F_RCNN_dataset\\val\\val_annotations.json\"\n",
    "\n",
    "# Path to the root directory of the training images\n",
    "train_image_root = r\"C:\\Users\\gokul\\Desktop\\CSIS_SS23\\F_RCNN_dataset\\train\\images\"\n",
    "\n",
    "# Path to the root directory of the validation images\n",
    "val_image_root = r\"C:\\Users\\gokul\\Desktop\\CSIS_SS23\\F_RCNN_dataset\\val\\images\"\n",
    "\n",
    "# Define the number of classes (including the background class)\n",
    "num_classes = 2\n",
    "\n",
    "# Define the batch size for training\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_root, annotations_path, transforms=None):\n",
    "        self.image_root = image_root\n",
    "        self.annotations_path = annotations_path\n",
    "        self.transforms = transforms\n",
    "        self.annotations = self.load_annotations()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.annotations[idx]['image_path']\n",
    "        targets = self.annotations[idx]['targets']\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply any necessary transformations\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "    def load_annotations(self):\n",
    "        with open(self.annotations_path, 'r') as f:\n",
    "            annotations = json.load(f)\n",
    "\n",
    "        # Process annotations and return the dataset\n",
    "        dataset = []\n",
    "        for annotation in annotations['annotations']:\n",
    "            image_filename = str(annotation['image_id'])\n",
    "            image_path = self.image_root + image_filename\n",
    "            targets = annotation['category_id']  # Modify the key based on your JSON structure\n",
    "            dataset.append({'image_path': image_path, 'targets': targets})\n",
    "\n",
    "        return dataset\n",
    "\n",
    "transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Create the training dataset instance\n",
    "train_dataset = CustomDataset(train_image_root, train_annotations_path, transforms=transform)\n",
    "\n",
    "# Create the validation dataset instance\n",
    "val_dataset = CustomDataset(val_image_root, val_annotations_path, transforms=transform)\n",
    "\n",
    "# Define the data loaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT')\n",
    "\n",
    "# Replace the classifier with a new one\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "# Define the number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "train_iterator = iter(train_loader)\n",
    "batch_images, batch_targets = next(train_iterator)\n",
    "\n",
    "train_iterator = iter(train_loader)\n",
    "batch_images, batch_targets = next(train_iterator)\n",
    "\n",
    "# Print the shape of the first few images and targets in the batch\n",
    "num_samples = min(5, len(batch_images))  # Adjust the number of samples to display\n",
    "for i in range(num_samples):\n",
    "    print(f\"Image Shape: {batch_images[i].shape}\")\n",
    "    print(f\"Targets: {batch_targets[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf693894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Print current epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    \n",
    "    # Training\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        print('Entered into this loop')\n",
    "        # Move the images and targets to the GPU\n",
    "        images = [F.to_tensor(img).to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training status\n",
    "        print(f\"Batch [{batch_idx+1}/{len(train_loader)}], Loss: {losses.item():.4f}\")\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(val_loader):\n",
    "            # Move the images and targets to the GPU\n",
    "            images = [F.to_tensor(img).to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            model(images, targets)\n",
    "\n",
    "            # Print validation status\n",
    "            print(f\"Validation Batch [{batch_idx+1}/{len(val_loader)}]\")\n",
    "\n",
    "        # Perform any necessary evaluation or logging for validation\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    torch.save(model.state_dict(), f'faster_rcnn_epoch_{epoch + 1}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07db27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
